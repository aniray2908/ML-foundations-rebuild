# Supervised Learning

This module covers classical supervised learning methods for regression and classification.

Supervised learning answers:

> Given input features X, predict target Y.

This section develops a structured understanding of:

- Model capacity
- Biasâ€“variance tradeoff
- Regularization
- Model selection
- Margin-based classification
- Evaluation under class imbalance
- Diagnostic reasoning
- Generalization behavior

---

## ðŸ“š Structure

### Theory

All conceptual documentation is located in:

- supervised_learning/theory/

This includes:

- Linear regression
- Regularization (Ridge, Lasso)
- Model selection (Cross-validation, AIC/AICc)
- Logistic regression
- Decision trees and ensembles
- Support Vector Machines
- Resampling methods (Bootstrap)
- Learning curves
- Biasâ€“variance diagnostics

---

### Demonstrations

All implementation notebooks are located in:

- supervised_learning/demos/

These notebooks provide:

- Visual biasâ€“variance analysis
- Regularization comparison
- Cross-validation tuning
- Model comparison experiments
- Tree vs Random Forest visualization
- ROC & Precisionâ€“Recall analysis
- Bootstrap variance estimation
- Learning curve diagnostics
- SVM boundary visualization

---

## Key Learning Outcome

After completing this module:

- Biasâ€“variance tradeoff is internalized.
- Regularization mechanics are understood.
- Linear and nonlinear classifiers are structurally compared.
- Margin maximization and kernel methods are understood conceptually.
- Model evaluation reflects class imbalance and cost asymmetry.
- Learning curves guide model and data decisions.

The classical supervised learning core is complete.
