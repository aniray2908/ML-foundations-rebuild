# Statistical Learning

This section focuses on understanding model behavior, regularization, classification, ensemble methods, and disciplined model evaluation.

The goal is to move from:

‚ÄúFitting models‚Äù

to

‚ÄúUnderstanding generalization, model complexity, and decision impact.‚Äù

---

## üìö Theory Modules

- 01_bias_variance.md  
- 02_linear_regression.md  
- 03_regularization.md  
- 04_model_selection.md  
- 05_logistic_regression.md  
- 06_trees_and_ensembles.md  
- 07_model_evaluation.md  
- conceptual_derivations.md  

---

## These documents cover:

### Regression
- Bias‚Äìvariance tradeoff
- OLS and multicollinearity
- Ridge & Lasso mechanics

### Model Selection
- Cross-validation
- AIC & AICc
- Model selection philosophy

### Classification
- Logistic regression and log-odds modeling
- Cross-entropy loss
- Linear decision boundaries

### Nonlinear Models
- Decision trees (Gini & Entropy intuition)
- Tree instability and overfitting

### Ensemble Learning
- Bagging
- Random Forest
- Variance reduction through decorrelation

### Model Evaluation
- Confusion matrix
- Precision, Recall, F1
- ROC vs Precision‚ÄìRecall analysis
- Threshold tradeoffs
- Calibration and probability reliability
- Cost-sensitive reasoning

---

## üìì Implementation Notebooks

- 01_bias_variance_demo.ipynb  
- 02_ridge_vs_lasso.ipynb  
- 03_cross_validation_and_hyperparameter_tuning.ipynb  
- 04_model_comparison.ipynb  
- 05_logistic_regression_demo.ipynb  
- 06_tree_vs_random_forest_demo.ipynb  
- 07_model_evaluation_demo.ipynb  

These notebooks demonstrate:

- Visual bias‚Äìvariance behavior
- Coefficient shrinkage and sparsity
- Hyperparameter tuning using K-fold CV
- Structured model comparison
- Linear vs nonlinear classification boundaries
- Ensemble variance reduction
- ROC, PR, and calibration analysis

---

## Key Learning Outcome

After completing this module:

- Bias‚Äìvariance reasoning is internalized.
- Regularization mechanics are understood mathematically and intuitively.
- Linear and nonlinear classifiers are structurally compared.
- Ensemble variance reduction is understood conceptually.
- Evaluation metrics are chosen based on class imbalance and cost asymmetry.
- Calibration and discrimination are clearly distinguished.

Statistical learning foundations are complete.

---

## References

- [Introduction to Statistical Learning (ISLR)](https://www.statlearning.com/)
- [The Elements of Statistical Learning (ESL)](https://web.stanford.edu/~hastie/ElemStatLearn/)
- [Scikit-learn Documentation](https://scikit-learn.org/stable/supervised_learning.html)
